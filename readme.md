# 1D CNN From scratch (NumPy)

My OOP implementation of a 1D CNN (NumPy) with forward propagation and backpropagation (gradients computed by chain-rule).

> **Note:** This is not a PyTorch or TensorFlow implementation â€” all layers, activations, loss functions, and gradient computations are manually coded for educational purpose.

---

## ğŸš€ Features

- **1D Convolution (`Conv1D`)**
- **Max & Average Pooling (`MaxPool1D`, `AvgPool1D`)**
- **Fully Connected Layers (`FCNN`)**
- **Activations:** ReLU, LeakyReLU, Sigmoid, Swish
- **Loss Functions:** Squared Error Loss (MSE), Binary Cross-Entropy (planned)
- **Forward and Backward Pass:** gradients computed by chain rule
- **Flatten Layer:** Reshapes feature maps for FCNN
- **Utility Function:** Padding
- **Enhancements:**  
  - Batching  
  - Support for multiple channels  
  - Xavier/He initialization  
  - Gradient clipping & numerical checks  
  - Experiment tracking (W&B)
  - Vectorization
  - Tensor implementation wth CUDA support

---

## ğŸ“ Repository Structure
```
â”œâ”€â”€ activations
â”‚   â”œâ”€â”€ LeakyReLU.py
â”‚   â”œâ”€â”€ ReLU.py
â”‚   â”œâ”€â”€ Sigmoid.py
â”‚   â””â”€â”€ Swish.py
â”œâ”€â”€ layers
â”‚   â”œâ”€â”€ AvgPool1D.py
â”‚   â”œâ”€â”€ Conv1D.py
â”‚   â”œâ”€â”€ FCNN.py
â”‚   â”œâ”€â”€ Flatten.py
â”‚   â”œâ”€â”€ MaxPool1D.py
â”œâ”€â”€ loss
â”‚   â”œâ”€â”€ BCELoss.py
â”‚   â”œâ”€â”€ MSELoss.py
â”‚   â””â”€â”€ SquaredErrorLoss.py
â”œâ”€â”€ main.py
â”œâ”€â”€ readme.md
â”œâ”€â”€ simple_cnn.py
â””â”€â”€ utils
    â”œâ”€â”€ pad_input.py
    â”œâ”€â”€ plot_loss_curve.py
    â”œâ”€â”€ printv.py
```

## âš¡ Usage Example

A working example is provided in simple_cnn.py

### Installation

1. Clone this repo: ```git clone https://github.com/rizanB/from_scratch.git```
2. Install dependencies with pip or conda: ```numpy matplotlib timeit```